{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Multi-Class Text Classification with BERT üöÄ\n","\n","[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/downloads/release)\n","[![PyTorch Version](https://img.shields.io/badge/pytorch-1.8%2B-orange)](https://pytorch.org/get-started/locally/)\n","\n","## Project Overview\n","\n","### üè¢ Business Overview\n","In this NLP project, we aim to perform multiclass text classification using a pre-trained BERT model. \n","\n","### üéØ Aim\n","The goal is to leverage the power of the BERT (Bidirectional Encoder Representations) model, an open-source ML framework for Natural Language Processing, to achieve state-of-the-art results in multiclass text classification.\n","\n","## Data Description\n","\n","The dataset includes customer complaints about financial products, with columns for complaint text and product labels. The task is to predict the product category based on the complaint text.\n","\n","## Tech Stack\n","\n","- **Language:** Python\n","- **Libraries:** pandas, torch, nltk, numpy, pickle, re, tqdm, sklearn, transformers\n","\n","## Prerequisite\n","\n","1. Install the torch framework\n","2. Understanding of Multiclass Text Classification using Naive Bayes\n","3. Familiarity with Skip Gram Model for Word Embeddings\n","4. Knowledge of building Multi-Class Text Classification Models with RNN and LSTM\n","5. Understanding Text Classification Model with Attention Mechanism in NLP\n","\n","## Approach\n","\n","1. **Data Processing**\n","   - Read CSV, handle null values, encode labels, preprocess text.\n","\n","2. **Model Building**\n","   - Create BERT model, define dataset, train and test functions.\n","\n","3. **Training**\n","   - Load data, split, create datasets and loaders.\n","   - Train BERT model on GPU/CPU.\n","\n","4. **Predictions**\n","   - Make predictions on new text data.\n","\n","## Project Structure\n","\n","- **Input:** complaints.csv\n","- **Output:** bert_pre_trained.pth, label_encoder.pkl, labels.pkl, tokens.pkl\n","- **Source:** model.py, data.py, utils.py\n","- **Files:** Engine.py, bert.ipynb, processing.py, predict.py, README.md, requirements.txt\n","\n","## Takeaways\n","\n","1. Solving business problems using pre-trained models.\n","2. Leveraging BERT for text classification.\n","3. Data preparation and model training.\n","4. Making predictions on new data.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/admin/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2024-11-11 21:03:22.434590: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-11 21:03:22.570337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1731339202.637467    7554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1731339202.656461    7554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-11 21:03:22.775148: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import re\n","import torch\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch.nn as nn\n","from transformers import BertModel\n","from transformers import BertTokenizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":6,"id":"1fb7bb17","metadata":{},"outputs":[],"source":["df = pd.read_csv('../../processed_data/full_data.csv')  \n"]},{"cell_type":"code","execution_count":8,"id":"81b863cc","metadata":{},"outputs":[{"data":{"text/plain":["array(['cyber bullying/stalking/sexting', 'fraud call/vishing',\n","       'online gambling  betting', 'online job fraud',\n","       'upi related frauds', 'internet banking related fraud',\n","       'rape/gang rape-sexually abusive content', 'other',\n","       'profile hacking identity theft',\n","       'debit/credit card fraud or sim swap fraud',\n","       'ewallet related fraud', 'data breach/theft',\n","       'cheating by impersonation',\n","       'denial of service (dos)/distributed denial of service (ddos) attacks',\n","       'fakeimpersonating profile', 'cryptocurrency fraud',\n","       'sexually explicit act', 'sexually obscene material',\n","       'malware attack', 'business email compromise/email takeover',\n","       'email hacking', 'hacking/defacement',\n","       'unauthorised access/data breach', 'sql injection',\n","       'provocative speech for unlawful acts', 'ransomware attack',\n","       'cyber terrorism',\n","       'child pornography/child sexual abuse material (csam)',\n","       'tampering with computer source documents',\n","       'demat/depository fraud', 'online trafficking',\n","       'online matrimonial fraud', 'defacement/hacking',\n","       'damage to computer systems', 'impersonating email',\n","       'email phishing', 'ransomware', 'intimidating email',\n","       'against interest of sovereignty or integrity of india',\n","       'computer generated csam/csem', 'cyber blackmailing & threatening',\n","       'sexual harassment'], dtype=object)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df['sub_category'].unique()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["lr = 1e-3\n","seq_len = 20\n","dropout = 0.5\n","num_epochs = 10\n","label_col = \"sub_category\"\n","tokens_path = \"tokens.pkl\"\n","labels_path = \"labels.pkl\"\n","data_path = \"../../processed_data/full_data.csv\"\n","model_path = \"bert_pre_trained.pth\"\n","text_col_name = \"crimeaditionalinfo\"\n","label_encoder_path = \"label_encoder.pkl\"\n","product_map = {'Vehicle loan or lease': 'vehicle_loan',\n","               'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n","               'Credit card or prepaid card': 'card',\n","               'Money transfer, virtual currency, or money service': 'money_transfer',\n","               'virtual currency': 'money_transfer',\n","               'Mortgage': 'mortgage',\n","               'Payday loan, title loan, or personal loan': 'loan',\n","               'Debt collection': 'debt_collection',\n","               'Checking or savings account': 'savings_account',\n","               'Credit card': 'card',\n","               'Bank account or service': 'savings_account',\n","               'Credit reporting': 'credit_report',\n","               'Prepaid card': 'card',\n","               'Payday loan': 'loan',\n","               'Other financial service': 'others',\n","               'Virtual currency': 'money_transfer',\n","               'Student loan': 'loan',\n","               'Consumer Loan': 'loan',\n","               'Money transfers': 'money_transfer'}"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def save_file(name, obj):\n","    \"\"\"\n","    Function to save an object as pickle file\n","    \"\"\"\n","    with open(name, 'wb') as f:\n","        pickle.dump(obj, f)\n","\n","\n","def load_file(name):\n","    \"\"\"\n","    Function to load a pickle object\n","    \"\"\"\n","    return pickle.load(open(name, \"rb\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Process text data\n","---"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["data = df"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["data.dropna(subset=[text_col_name], inplace=True)"]},{"cell_type":"code","execution_count":20,"id":"7fb4d62c","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category</th>\n","      <th>sub_category</th>\n","      <th>crimeaditionalinfo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>online and social media related crime</td>\n","      <td>cyber bullying/stalking/sexting</td>\n","      <td>i had continue received random calls and abusi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>online financial fraud</td>\n","      <td>fraud call/vishing</td>\n","      <td>the above fraudster is continuously messaging ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>online gambling  betting</td>\n","      <td>online gambling  betting</td>\n","      <td>he is acting like a police and demanding for m...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>online and social media related crime</td>\n","      <td>online job fraud</td>\n","      <td>in apna job i have applied for job interview f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>online financial fraud</td>\n","      <td>fraud call/vishing</td>\n","      <td>i received a call from lady stating that she w...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>124882</th>\n","      <td>online and social media related crime</td>\n","      <td>online matrimonial fraud</td>\n","      <td>a lady named rashmi probably a fake name had c...</td>\n","    </tr>\n","    <tr>\n","      <th>124883</th>\n","      <td>online financial fraud</td>\n","      <td>internet banking related fraud</td>\n","      <td>i am mr chokhe ram  two pers mobile number wer...</td>\n","    </tr>\n","    <tr>\n","      <th>124884</th>\n","      <td>any other cyber crime</td>\n","      <td>other</td>\n","      <td>mai bibekbraj maine pahle ki complain kar chuk...</td>\n","    </tr>\n","    <tr>\n","      <th>124885</th>\n","      <td>online financial fraud</td>\n","      <td>internet banking related fraud</td>\n","      <td>received url link for updating kyc from mobile...</td>\n","    </tr>\n","    <tr>\n","      <th>124886</th>\n","      <td>any other cyber crime</td>\n","      <td>other</td>\n","      <td>i saw add on facebook for job placement and i ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>124887 rows √ó 3 columns</p>\n","</div>"],"text/plain":["                                     category  \\\n","0       online and social media related crime   \n","1                      online financial fraud   \n","2                    online gambling  betting   \n","3       online and social media related crime   \n","4                      online financial fraud   \n","...                                       ...   \n","124882  online and social media related crime   \n","124883                 online financial fraud   \n","124884                  any other cyber crime   \n","124885                 online financial fraud   \n","124886                  any other cyber crime   \n","\n","                           sub_category  \\\n","0       cyber bullying/stalking/sexting   \n","1                    fraud call/vishing   \n","2              online gambling  betting   \n","3                      online job fraud   \n","4                    fraud call/vishing   \n","...                                 ...   \n","124882         online matrimonial fraud   \n","124883   internet banking related fraud   \n","124884                            other   \n","124885   internet banking related fraud   \n","124886                            other   \n","\n","                                       crimeaditionalinfo  \n","0       i had continue received random calls and abusi...  \n","1       the above fraudster is continuously messaging ...  \n","2       he is acting like a police and demanding for m...  \n","3       in apna job i have applied for job interview f...  \n","4       i received a call from lady stating that she w...  \n","...                                                   ...  \n","124882  a lady named rashmi probably a fake name had c...  \n","124883  i am mr chokhe ram  two pers mobile number wer...  \n","124884  mai bibekbraj maine pahle ki complain kar chuk...  \n","124885  received url link for updating kyc from mobile...  \n","124886  i saw add on facebook for job placement and i ...  \n","\n","[124887 rows x 3 columns]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# data.replace({label_col: product_map}, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Encode labels"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["label_encoder = LabelEncoder()\n","label_encoder.fit(data[label_col])\n","labels = label_encoder.transform(data[label_col])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["save_file(labels_path, labels)\n","save_file(label_encoder_path, label_encoder)"]},{"cell_type":"markdown","metadata":{},"source":["### Process the text column"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["input_text = list(data[text_col_name])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["124887"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["len(input_text)"]},{"cell_type":"markdown","metadata":{},"source":["### Convert text to lower case"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/124887 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [00:00<00:00, 940261.51it/s]\n"]}],"source":["input_text = [i.lower() for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Remove punctuations except apostrophe"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/124887 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [00:01<00:00, 115211.32it/s]\n"]}],"source":["input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i)\n","             for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Remove digits"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:1: SyntaxWarning: invalid escape sequence '\\d'\n","<>:1: SyntaxWarning: invalid escape sequence '\\d'\n","/tmp/ipykernel_7554/1209617680.py:1: SyntaxWarning: invalid escape sequence '\\d'\n","  input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [00:00<00:00, 151811.24it/s]\n"]}],"source":["input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Remove more than one consecutive instance of 'x'"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [00:00<00:00, 207912.22it/s]\n"]}],"source":["input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Remove multiple spaces with single space"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [00:02<00:00, 58427.91it/s]\n"]}],"source":["input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenize the text"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["'i had continue received random calls and abusive messages in my whatsapp someone added my number in a unknown facebook group name with only girls and still getting calls from unknown numbers pls help me and sort out the issue as soon as possible thank you'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["input_text[0]"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["sample_tokens = tokenizer(input_text[0], padding=\"max_length\",\n","                         max_length=seq_len, truncation=True,\n","                         return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[  101,   178,  1125,  2760,  1460,  7091,  3675,  1105, 22898,  7416,\n","          1107,  1139,  1184,  3202,  8661,  1800,  1896,  1139,  1295,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["sample_tokens"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[  101,   178,  1125,  2760,  1460,  7091,  3675,  1105, 22898,  7416,\n","          1107,  1139,  1184,  3202,  8661,  1800,  1896,  1139,  1295,   102]])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["sample_tokens[\"input_ids\"]"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["sample_tokens[\"attention_mask\"]"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124887/124887 [01:38<00:00, 1266.55it/s]\n"]}],"source":["tokens = [tokenizer(i, padding=\"max_length\", max_length=seq_len, \n","                    truncation=True, return_tensors=\"pt\") \n","         for i in tqdm(input_text)]"]},{"cell_type":"markdown","metadata":{},"source":["### Save the tokens"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["save_file(tokens_path, tokens)"]},{"cell_type":"markdown","metadata":{},"source":["## Create Bert model\n","---"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["class BertClassifier(nn.Module):\n","    \n","    def __init__(self, dropout, num_classes):\n","        super(BertClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","        for param in self.bert.parameters():\n","            param.required_grad = False\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, num_classes)\n","        self.activation = nn.ReLU()\n","    \n","    def forward(self, input_ids, attention_mask):\n","        _, bert_output = self.bert(input_ids=input_ids,\n","                                  attention_mask=attention_mask,\n","                                  return_dict=False)\n","        dropout_output = self.activation(self.dropout(bert_output))\n","        final_output = self.linear(dropout_output)\n","        return final_output"]},{"cell_type":"markdown","metadata":{},"source":["## Create PyTorch Dataset\n","---"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["class TextDataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, tokens, labels):\n","        self.tokens = tokens\n","        self.labels = labels\n","        \n","    def __len__(self):\n","        return len(self.tokens)\n","    \n","    def __getitem__(self, idx):\n","        return self.labels[idx], self.tokens[idx]"]},{"cell_type":"markdown","metadata":{},"source":["### Function to train the model"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def train(train_loader, valid_loader, model, criterion, optimizer, \n","          device, num_epochs, model_path):\n","    \"\"\"\n","    Function to train the model\n","    :param train_loader: Data loader for train dataset\n","    :param valid_loader: Data loader for validation dataset\n","    :param model: Model object\n","    :param criterion: Loss function\n","    :param optimizer: Optimizer\n","    :param device: CUDA or CPU\n","    :param num_epochs: Number of epochs\n","    :param model_path: Path to save the model\n","    \"\"\"\n","    best_loss = 1e8\n","    for i in range(num_epochs):\n","        print(f\"Epoch {i+1} of {num_epochs}\")\n","        valid_loss, train_loss = [], []\n","        model.train()\n","        # Train loop\n","        for batch_labels, batch_data in tqdm(train_loader):\n","            input_ids = batch_data[\"input_ids\"]\n","            attention_mask = batch_data[\"attention_mask\"]\n","            # Move data to GPU if available\n","            batch_labels = batch_labels.to(device)\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            input_ids = torch.squeeze(input_ids, 1)\n","            # Forward pass\n","            batch_output = model(input_ids, attention_mask)\n","            batch_output = torch.squeeze(batch_output)\n","            # Calculate loss\n","            ###batch_labels = batch_labels.type(torch.LongTensor)\n","            loss = criterion(batch_output, batch_labels)\n","            train_loss.append(loss.item())\n","            optimizer.zero_grad()\n","            # Backward pass\n","            loss.backward()\n","            # Gradient update step\n","            optimizer.step()\n","        model.eval()\n","        # Validation loop\n","        for batch_labels, batch_data in tqdm(valid_loader):\n","            input_ids = batch_data[\"input_ids\"]\n","            attention_mask = batch_data[\"attention_mask\"]\n","            # Move data to GPU if available\n","            batch_labels = batch_labels.to(device)\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            input_ids = torch.squeeze(input_ids, 1)\n","            # Forward pass\n","            batch_output = model(input_ids, attention_mask)\n","            batch_output = torch.squeeze(batch_output)\n","            # Calculate loss\n","            ###batch_labels = batch_labels.type(torch.LongTensor)\n","            loss = criterion(batch_output, batch_labels)\n","            valid_loss.append(loss.item())\n","        t_loss = np.mean(train_loss)\n","        v_loss = np.mean(valid_loss)\n","        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n","        if v_loss < best_loss:\n","            best_loss = v_loss\n","            # Save model if validation loss improves\n","            torch.save(model.state_dict(), model_path)\n","        print(f\"Best Validation Loss: {best_loss}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Function to test the model"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def test(test_loader, model, criterion, device):\n","    \"\"\"\n","    Function to test the model\n","    :param test_loader: Data loader for test dataset\n","    :param model: Model object\n","    :param criterion: Loss function\n","    :param device: CUDA or CPU\n","    \"\"\"\n","    model.eval()\n","    test_loss = []\n","    test_accu = []\n","    for batch_labels, batch_data in tqdm(test_loader):\n","        input_ids = batch_data[\"input_ids\"]\n","        attention_mask = batch_data[\"attention_mask\"]\n","        # Move data to GPU if available\n","        batch_labels = batch_labels.to(device)\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        input_ids = torch.squeeze(input_ids, 1)\n","        # Forward pass\n","        batch_output = model(input_ids, attention_mask)\n","        batch_output = torch.squeeze(batch_output)\n","        # Calculate loss\n","        ###batch_labels = batch_labels.type(torch.LongTensor)\n","        loss = criterion(batch_output, batch_labels)\n","        test_loss.append(loss.item())\n","        batch_preds = torch.argmax(batch_output, axis=1)\n","        # Move predictions to CPU\n","        if torch.cuda.is_available():\n","            batch_labels = batch_labels.cpu()\n","            batch_preds = batch_preds.cpu()\n","        # Compute accuracy\n","        test_accu.append(accuracy_score(batch_labels.detach().\n","                                        numpy(),\n","                                        batch_preds.detach().\n","                                        numpy()))\n","    test_loss = np.mean(test_loss)\n","    test_accu = np.mean(test_accu)\n","    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accu}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Train Bert model\n","---"]},{"cell_type":"markdown","metadata":{},"source":["### Load the files"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["tokens = load_file(tokens_path)\n","labels = load_file(labels_path)\n","label_encoder = load_file(label_encoder_path)\n","num_classes = len(label_encoder.classes_)"]},{"cell_type":"code","execution_count":44,"id":"7df853de","metadata":{},"outputs":[{"data":{"text/plain":["42"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["num_classes"]},{"cell_type":"code","execution_count":null,"id":"acee121a","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Split data into train, validation and test sets"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(tokens, labels,\n","                                                   test_size=0.2)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, \n","                                                      y_train,\n","                                                     test_size=0.25)"]},{"cell_type":"markdown","metadata":{},"source":["### Create PyTorch datasets"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["train_dataset = TextDataset(X_train, y_train)\n","valid_dataset = TextDataset(X_valid, y_valid)\n","test_dataset = TextDataset(X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Create data loaders"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=16,\n","                                           shuffle=True,\n","                                           drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                           batch_size=16)\n","test_loader = torch.utils.data.DataLoader(test_dataset, \n","                                         batch_size=16)"]},{"cell_type":"markdown","metadata":{},"source":["### Create model object"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n","                     else \"cpu\")"]},{"cell_type":"code","execution_count":49,"id":"72c737a1","metadata":{},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["model = BertClassifier(dropout, num_classes)"]},{"cell_type":"code","execution_count":null,"id":"a3f0ec76","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Define loss function and optimizer"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"markdown","metadata":{},"source":["### Move the model to GPU if available"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["if torch.cuda.is_available():\n","    model = model.cuda()\n","    criterion = criterion.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["### Training loop"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 of 10\n"]},{"name":"stderr","output_type":"stream","text":["  6%|‚ñå         | 270/4683 [06:21<1:44:00,  1.41s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[41], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, device, num_epochs, model_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Gradient update step\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train(train_loader, valid_loader, model, criterion, optimizer,\n","     device, num_epochs, model_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Test the model"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:57<00:00,  1.73it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 1.6601403439044953, Test Accuracy: 0.453125\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test(test_loader, model, criterion, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Predict on new text\n","---"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["input_text = '''I am a victim of Identity Theft & currently have an Experian account that \n","I can view my Experian Credit Report and getting notified when there is activity on \n","my Experian Credit Report. For the past 3 days I've spent a total of approximately 9 \n","hours on the phone with Experian. Every time I call I get transferred repeatedly and \n","then my last transfer and automated message states to press 1 and leave a message and \n","someone would call me. Every time I press 1 I get an automatic message stating than you \n","before I even leave a message and get disconnected. I call Experian again, explain what \n","is happening and the process begins again with the same end result. I was trying to have \n","this issue attended and resolved informally but I give up after 9 hours. There are hard \n","hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall \n","and I respectfully request that Experian remove the hard hit inquiries immediately just \n","like they've done in the past when I was able to speak to a live Experian representative \n","in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX \n","XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX \n","XX/XX/XXXX'''"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["input_text = input_text.lower()\n","input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n","input_text = re.sub(\"\\d+\", \"\", input_text)\n","input_text = re.sub(r'[x]{2,}', \"\", input_text)\n","input_text = re.sub(' +', ' ', input_text)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["tokens = tokenizer(input_text, padding=\"max_length\",\n","                 max_length=seq_len, truncation=True,\n","                 return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["input_ids = tokens[\"input_ids\"]\n","attention_mask = tokens[\"attention_mask\"]"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n","                     else \"cpu\")"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["input_ids = input_ids.to(device)\n","attention_mask = attention_mask.to(device)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["input_ids = torch.squeeze(input_ids, 1)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["label_encoder = load_file(label_encoder_path)\n","num_classes = len(label_encoder.classes_)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Class: credit_report\n"]}],"source":["# Create model object\n","model = BertClassifier(dropout, num_classes)\n","\n","# Load trained weights\n","model.load_state_dict(torch.load(model_path))\n","\n","# Move the model to GPU if available\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    \n","# Forward pass\n","out = torch.squeeze(model(input_ids, attention_mask))\n","\n","# Find predicted class\n","prediction = label_encoder.classes_[torch.argmax(out)]\n","print(f\"Predicted Class: {prediction}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}
