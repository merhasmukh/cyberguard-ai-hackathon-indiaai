{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:13:28.322221: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-15 18:13:28.332776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731674608.346223   15120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731674608.350376   15120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 18:13:28.363651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to /home/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/admin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv('../../processed_data/full_data.csv')  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Text Cleaning and Tokenization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    # Lemmatization\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sub_category\n",
       "upi related frauds                                                      35729\n",
       "other                                                                   14547\n",
       "debit/credit card fraud or sim swap fraud                               14357\n",
       "internet banking related fraud                                          11844\n",
       "fraud call/vishing                                                       7628\n",
       "cyber bullying/stalking/sexting                                          5455\n",
       "ewallet related fraud                                                    5385\n",
       "rape/gang rape-sexually abusive content                                  3734\n",
       "fakeimpersonating profile                                                3062\n",
       "profile hacking identity theft                                           2823\n",
       "cheating by impersonation                                                2706\n",
       "sexually obscene material                                                2503\n",
       "sexually explicit act                                                    2087\n",
       "unauthorised access/data breach                                          1484\n",
       "online job fraud                                                         1206\n",
       "demat/depository fraud                                                    983\n",
       "tampering with computer source documents                                  761\n",
       "hacking/defacement                                                        740\n",
       "ransomware attack                                                         720\n",
       "denial of service (dos)/distributed denial of service (ddos) attacks      691\n",
       "malware attack                                                            691\n",
       "sql injection                                                             675\n",
       "data breach/theft                                                         655\n",
       "cryptocurrency fraud                                                      646\n",
       "online gambling  betting                                                  578\n",
       "provocative speech for unlawful acts                                      547\n",
       "child pornography/child sexual abuse material (csam)                      502\n",
       "email hacking                                                             479\n",
       "business email compromise/email takeover                                  380\n",
       "online trafficking                                                        244\n",
       "cyber terrorism                                                           213\n",
       "email phishing                                                            211\n",
       "online matrimonial fraud                                                  170\n",
       "damage to computer systems                                                147\n",
       "defacement/hacking                                                        128\n",
       "ransomware                                                                 74\n",
       "impersonating email                                                        57\n",
       "intimidating email                                                         40\n",
       "computer generated csam/csem                                                2\n",
       "against interest of sovereignty or integrity of india                       1\n",
       "cyber blackmailing & threatening                                            1\n",
       "sexual harassment                                                           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sub_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124887, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crimeaditionalinfo'] = df['crimeaditionalinfo'].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    continue received random call abusive message ...\n",
       "1    fraudster continuously messaging asking pay mo...\n",
       "2    acting like police demanding money adding sect...\n",
       "3    apna job applied job interview telecalling res...\n",
       "4    received call lady stating send new phone vivo...\n",
       "Name: crimeaditionalinfo, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['crimeaditionalinfo'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 132377\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['crimeaditionalinfo'])\n",
    "\n",
    "# Get all unique words (vocabulary)\n",
    "unique_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the unique words\n",
    "print(\"Total unique words:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['crimeaditionalinfo']\n",
    "y=df['sub_category']\n",
    "# Encode category and subcategory labels\n",
    "label_encoder_sub_category = LabelEncoder()\n",
    "y_sub_category = label_encoder_sub_category.fit_transform(y)\n",
    "ros = RandomOverSampler(sampling_strategy='not majority', random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X.values.reshape(-1, 1), y_sub_category)\n",
    "X_resampled = X_resampled.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess text data\n",
    "max_words = 200000\n",
    "max_len = 500\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
    "tokenizer.fit_on_texts(X_resampled)\n",
    "\n",
    "X_seq= tokenizer.texts_to_sequences(X_resampled)\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_sub_cat = to_categorical(y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_resampled),\n",
    "    y=y_resampled\n",
    ")\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: np.float64(1.0),\n",
       " 1: np.float64(1.0),\n",
       " 2: np.float64(1.0),\n",
       " 3: np.float64(1.0),\n",
       " 4: np.float64(1.0),\n",
       " 5: np.float64(1.0),\n",
       " 6: np.float64(1.0),\n",
       " 7: np.float64(1.0),\n",
       " 8: np.float64(1.0),\n",
       " 9: np.float64(1.0),\n",
       " 10: np.float64(1.0),\n",
       " 11: np.float64(1.0),\n",
       " 12: np.float64(1.0),\n",
       " 13: np.float64(1.0),\n",
       " 14: np.float64(1.0),\n",
       " 15: np.float64(1.0),\n",
       " 16: np.float64(1.0),\n",
       " 17: np.float64(1.0),\n",
       " 18: np.float64(1.0),\n",
       " 19: np.float64(1.0),\n",
       " 20: np.float64(1.0),\n",
       " 21: np.float64(1.0),\n",
       " 22: np.float64(1.0),\n",
       " 23: np.float64(1.0),\n",
       " 24: np.float64(1.0),\n",
       " 25: np.float64(1.0),\n",
       " 26: np.float64(1.0),\n",
       " 27: np.float64(1.0),\n",
       " 28: np.float64(1.0),\n",
       " 29: np.float64(1.0),\n",
       " 30: np.float64(1.0),\n",
       " 31: np.float64(1.0),\n",
       " 32: np.float64(1.0),\n",
       " 33: np.float64(1.0),\n",
       " 34: np.float64(1.0),\n",
       " 35: np.float64(1.0),\n",
       " 36: np.float64(1.0),\n",
       " 37: np.float64(1.0),\n",
       " 38: np.float64(1.0),\n",
       " 39: np.float64(1.0),\n",
       " 40: np.float64(1.0),\n",
       " 41: np.float64(1.0)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500618, 42)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_pad, y_sub_cat, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/Desktop/AI_Hackathon/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,730</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m25,600,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │         \u001b[38;5;34m2,730\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,656,298</span> (97.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,656,298\u001b[0m (97.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,656,298</span> (97.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,656,298\u001b[0m (97.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m37516/37516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13816s\u001b[0m 368ms/step - accuracy: 0.5350 - loss: 1.4408 - val_accuracy: 0.7762 - val_loss: 0.5888\n",
      "Epoch 2/5\n",
      "\u001b[1m37516/37516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13616s\u001b[0m 363ms/step - accuracy: 0.7747 - loss: 0.6012 - val_accuracy: 0.8036 - val_loss: 0.5003\n",
      "Epoch 3/5\n",
      "\u001b[1m37516/37516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13294s\u001b[0m 354ms/step - accuracy: 0.7997 - loss: 0.5197 - val_accuracy: 0.8125 - val_loss: 0.4777\n",
      "Epoch 4/5\n",
      "\u001b[1m37516/37516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11204s\u001b[0m 299ms/step - accuracy: 0.8088 - loss: 0.4919 - val_accuracy: 0.8167 - val_loss: 0.4652\n",
      "Epoch 5/5\n",
      "\u001b[1m37516/37516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11074s\u001b[0m 295ms/step - accuracy: 0.8143 - loss: 0.4743 - val_accuracy: 0.8204 - val_loss: 0.4584\n",
      "Validation Loss: 0.4584, Validation Accuracy: 0.8204\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define the model architecture\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(max_words, 128, input_length=max_len)(input_layer)\n",
    "lstm_layer = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "dense_layer = Dense(64, activation='relu')(lstm_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(len(np.unique(y_resampled)), activation='softmax')(dropout_layer)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # Suitable for multiclass classification\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in keras format\n",
    "\n",
    "model.save('sub_category_text_classification_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sub_category_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the label encoders to files\n",
    "with open('label_encoder_sub_category.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder_sub_category, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:13:39.508772: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "max_words = 200000\n",
    "max_len = 500\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('sub_category_text_classification_model.keras')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('sub_category_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load the label encoder\n",
    "with open('label_encoder_sub_category.pickle', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new data\n",
    "new_data = [\"\"\"hello sir and mam\n",
    "this very urgent and emergency  to inform you\n",
    "some one created a facebook fake account of my name and he demand for money of my facebook friends \n",
    "kindly request to you please take serious action on this matter\"\"\"]\n",
    "\n",
    "# Clean and preprocess\n",
    "new_data_cleaned = [clean_text(text) for text in new_data]\n",
    "\n",
    "# Convert text to sequences\n",
    "new_sequences = tokenizer.texts_to_sequences(new_data_cleaned)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "Text: ['hello sir mam urgent emergency inform one created facebook fake account name demand money facebook friend kindly request please take serious action matter']\n",
      "Predicted Sub Category: fakeimpersonating profile\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "# Convert predictions to label indices\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Decode indices to original category labels\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "\n",
    "# Display results\n",
    "for i, text in enumerate(new_data):\n",
    "    print(f\"Text: {new_data_cleaned}\")\n",
    "    print(f\"Predicted Sub Category: {predicted_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
